
from google.cloud import bigquery
from google.cloud import storage
import os

def load_parquet_to_bigquery(event, context):
    # 환경 변수에서 프로젝트 ID 및 데이터셋 ID를 가져옵니다.
    project_id = os.getenv('PROJECT_ID')
    dataset_id = os.getenv('DATASET_ID')
    table_id = os.getenv('TABLE_ID')

    # 이벤트에서 버킷 이름을 가져옵니다.
    bucket_name = 'dasc-s3-exam'
    file_prefix = 'AWS-Exam/'  # 전체 경로에서 탐색을 시작할 루트 디렉토리

    # Cloud Storage 클라이언트 생성
    storage_client = storage.Client()
    bucket = storage_client.bucket(bucket_name)

    # 경로에 있는 모든 파일을 가져옵니다
    blobs = list(bucket.list_blobs(prefix=file_prefix))
    if not blobs:
        print(f"No files found in path: {file_prefix}")
        return "No files found.", 404

    # "part-00000-"으로 시작하는 모든 파일을 찾습니다
    target_blobs = []
    for blob in blobs:
        if blob.name.endswith(".parquet"):  # 모든 Parquet 파일을 찾습니다
            target_blobs.append(blob)

    if not target_blobs:
        print("No matching Parquet files found.")
        return "No matching Parquet files found.", 404

    # BigQuery 클라이언트 생성
    bigquery_client = bigquery.Client()

    # 각 Parquet 파일을 BigQuery에 로드
    for target_blob in target_blobs:
        file_name = target_blob.name
        uri = f"gs://{bucket_name}/{file_name}"

        # BigQuery 테이블의 참조를 정의합니다.
        table_ref = bigquery_client.dataset(dataset_id).table(table_id)

        # BigQuery 작업을 구성합니다.
        job_config = bigquery.LoadJobConfig(
            source_format=bigquery.SourceFormat.PARQUET,
            autodetect=True,
        )

        # Cloud Storage에서 BigQuery로 데이터를 로드하는 작업을 시작합니다.
        load_job = bigquery_client.load_table_from_uri(
            uri,
            table_ref,
            job_config=job_config
        )

        # 작업이 완료될 때까지 대기합니다.
        load_job.result()

    return f"Loaded {len(target_blobs)} files into {table_id} in BigQuery", 200


TXT 파일 
google-cloud-bigquery==3.10.0
google-cloud-storage==2.11.0
